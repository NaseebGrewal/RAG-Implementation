{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Notebook Overview\n",
    "\n",
    "This notebook demonstrates the complete workflow of a simple RAG system designed to identify potentially suspicious organizations from a fictional dataset. The main steps include:\n",
    "\n",
    "1. **Dependency Installation:**  \n",
    "   Ensuring that all necessary Python packages are installed for the notebook to run smoothly.\n",
    "\n",
    "2. **Data Indexing:**  \n",
    "   Loading and preparing a dataset of organization descriptions.\n",
    "\n",
    "3. **Embedding Generation:**  \n",
    "   Using a pre-trained SentenceTransformer model to convert text descriptions into numerical embeddings.\n",
    "\n",
    "4. **Similarity Search:**  \n",
    "   Leveraging FAISS to perform efficient vector similarity searches between a user query and document embeddings.\n",
    "\n",
    "5. **Prompt Construction:**  \n",
    "   Building a query prompt by integrating the top-k retrieved document details.\n",
    "\n",
    "6. **Response Generation:**  \n",
    "   Using the Hugging Face Mistral API to generate human-like answers based on the constructed prompt.\n",
    "\n",
    "\n",
    "\n",
    "Follow the cells sequentially to install dependencies, index the data, generate embeddings, perform the similarity search, and finally produce a detailed, human-like response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS Index Creation and Data Embedding Details\n",
    "\n",
    "This cell contains key functions that enable the Retrieval-Augmented Generation (RAG) system to process and index the dataset. The main steps are:\n",
    "\n",
    "1. **Data Loading & Preprocessing:**\n",
    "   - **Function:** `load_and_preprocess_data`\n",
    "   - **Purpose:**  \n",
    "     Reads the raw dataset file from the specified `FILE_PATH` and parses it using a regular expression.  \n",
    "     It extracts each document's ID, title, and description, returning a list of dictionaries.\n",
    "\n",
    "2. **Embedding Generation:**\n",
    "   - **Function:** `embed_texts`\n",
    "   - **Purpose:**  \n",
    "     Uses a pre-trained SentenceTransformer model (default: `\"BAAI/bge-small-en-v1.5\"`) to generate vector embeddings for each document.  \n",
    "     The embeddings are computed from a concatenation of the document title and text, then returned as a NumPy array.\n",
    "\n",
    "3. **FAISS Index Construction:**\n",
    "   - **Function:** `create_faiss_index`\n",
    "   - **Purpose:**  \n",
    "     Combines the data loading and embedding functions to build a FAISS index with L2 distance (using `faiss.IndexFlatL2`).  \n",
    "     It indexes the embeddings and also creates a mapping between index positions and the original document dictionaries for later retrieval.\n",
    "\n",
    "> **Note:**  \n",
    "> Ensure the `.env` file is configured with the correct `FILE_PATH` to your dataset before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# Constants\n",
    "FILE_PATH = os.getenv(\"FILE_PATH\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset from the given file path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing documents.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries containing parsed document information.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read()\n",
    "\n",
    "    pattern = r\"Document (\\d+): (.*?)\\nDescription:\\n(.*?)(?=\\nDocument \\d+:|\\Z)\"\n",
    "    matches = re.findall(pattern, raw_text, re.DOTALL)\n",
    "\n",
    "    documents = [\n",
    "        {\n",
    "            \"doc_id\": int(doc_id),\n",
    "            \"title\": title.strip(),\n",
    "            \"text\": description.strip(),\n",
    "        }\n",
    "        for doc_id, title, description in matches\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "\n",
    "def embed_texts(\n",
    "    documents: List[Dict[str, Any]], model_name: str = \"BAAI/bge-small-en-v1.5\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed the texts using a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Dict[str, Any]]): List of documents with 'title' and 'text'.\n",
    "        model_name (str): Name of the SentenceTransformer model to use.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embeddings for the input documents.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    corpus = [f\"{doc['title']}: {doc['text']}\" for doc in documents]\n",
    "    embeddings = model.encode(corpus, show_progress_bar=True)\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "\n",
    "def create_faiss_index() -> Tuple[faiss.Index, Dict[int, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Create a FAISS index from the embedded document vectors.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[faiss.Index, Dict[int, Dict[str, Any]]]: \n",
    "            FAISS index and mapping from index ID to document.\n",
    "    \"\"\"\n",
    "    file_path = os.getenv(\"FILE_PATH\")\n",
    "    if not file_path:\n",
    "        raise ValueError(\"FILE_PATH environment variable is not set.\")\n",
    "\n",
    "    documents = load_and_preprocess_data(file_path)\n",
    "    embeddings = embed_texts(documents)\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    id_to_doc = {i: doc for i, doc in enumerate(documents)}\n",
    "\n",
    "    print(f\"FAISS index created with {index.ntotal} documents.\")\n",
    "    return index, id_to_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theowner/Documents/GitHub/Hawk-Submission/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "import sqlite3\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "FILE_PATH = os.getenv(\"FILE_PATH\")\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "INDEX_PATH = \"faiss_index.bin\"\n",
    "DB_PATH = \"documents.db\"\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Stream-load and preprocess dataset.\"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        buffer = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\"Document \") and buffer:\n",
    "                doc = parse_document(buffer)\n",
    "                if doc:\n",
    "                    documents.append(doc)\n",
    "                buffer = \"\"\n",
    "            buffer += line\n",
    "        if buffer:\n",
    "            doc = parse_document(buffer)\n",
    "            if doc:\n",
    "                documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def parse_document(raw_document: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse individual raw document.\"\"\"\n",
    "    match = re.match(r\"Document (\\d+): (.*?)\\nDescription:\\n(.*)\", raw_document, re.DOTALL)\n",
    "    if match:\n",
    "        doc_id, title, text = match.groups()\n",
    "        return {\n",
    "            \"doc_id\": int(doc_id),\n",
    "            \"title\": title.strip(),\n",
    "            \"text\": text.strip(),\n",
    "        }\n",
    "    return {}\n",
    "\n",
    "\n",
    "def embed_texts(documents: List[Dict[str, Any]], model_name: str = MODEL_NAME) -> np.ndarray:\n",
    "    \"\"\"Embed texts in batches to conserve memory.\"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "    batch_size = 64\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "        corpus = [f\"{doc['title']}: {doc['text']}\" for doc in batch_docs]\n",
    "        batch_embeddings = model.encode(corpus, show_progress_bar=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings).astype(np.float32)\n",
    "\n",
    "\n",
    "def create_faiss_index() -> Tuple[faiss.Index, Dict[int, Dict[str, Any]]]:\n",
    "    \"\"\"Create or load a FAISS index and store metadata in SQLite.\"\"\"\n",
    "    if not FILE_PATH:\n",
    "        raise ValueError(\"FILE_PATH environment variable is not set.\")\n",
    "\n",
    "    documents = load_and_preprocess_data(FILE_PATH)\n",
    "    embeddings = embed_texts(documents)\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        index = faiss.read_index(INDEX_PATH)\n",
    "        print(\"FAISS index loaded from disk.\")\n",
    "    else:\n",
    "        index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "        index.hnsw.efConstruction = 40\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, INDEX_PATH)\n",
    "        print(\"FAISS index created and saved.\")\n",
    "\n",
    "    # Store metadata\n",
    "    store_metadata(documents)\n",
    "\n",
    "    id_to_doc = {i: doc for i, doc in enumerate(documents)}\n",
    "    return index, id_to_doc\n",
    "\n",
    "\n",
    "def store_metadata(documents: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"Store document metadata persistently.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        doc_id INTEGER,\n",
    "        title TEXT,\n",
    "        text TEXT\n",
    "    )\"\"\")\n",
    "\n",
    "    cursor.executemany(\"\"\"\n",
    "        INSERT INTO documents (doc_id, title, text) VALUES (?, ?, ?)\n",
    "    \"\"\", [(doc['doc_id'], doc['title'], doc['text']) for doc in documents])\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def search_query(query: str, index: faiss.Index, model: SentenceTransformer, top_k: int = 5) -> List[int]:\n",
    "    \"\"\"Search query and return the top_k most relevant document indices.\"\"\"\n",
    "    query_embedding = model.encode([query]).astype(np.float32)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return indices[0].tolist()\n",
    "\n",
    "\n",
    "def retrieve_document(doc_idx: int) -> Dict[str, Any]:\n",
    "    \"\"\"Retrieve a single document's metadata by index from the database.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT doc_id, title, text FROM documents WHERE id=?\", (doc_idx+1,))\n",
    "    row = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return {\"doc_id\": row[0], \"title\": row[1], \"text\": row[2]} if row else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 14.77it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 22.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from disk.\n",
      "FAISS index now has 12 documents indexed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index, id_to_doc = create_faiss_index()\n",
    "print(f\"FAISS index now has {index.ntotal} documents indexed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "\"Tell me about Cascade Capital Management?\",\n",
    "\"Which organizations show signs of potential money laundering through complex structures?\",\n",
    "\"What irregular transaction patterns are identified for Aurora Financial Services, and why do these raise concerns about potential money laundering?\",\n",
    "\"How do the described transaction patterns of Blue Horizon Investments hint at possible insider trading or market manipulation?\",\n",
    "\"Compare the descriptions of Falcon Secure Bank and Helix Fintech Solutions. What aspects of their operations contribute to one being perceived as having higher transparency and regulatory compliance than the other?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_list = search_query(queries[0],index,SentenceTransformer(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents for query 'Tell me about Cascade Capital Management?':\n",
      "documents:\n",
      " [{'doc_id': 3, 'title': 'Cascade Capital Management', 'text': 'Cascade Capital Management, a venture capital firm specializing in tech investments, has grown rapidly but relies on a complex network of subsidiary shell companies. This structure has attracted regulatory scrutiny regarding transparency and compliance.'}, {'doc_id': 5, 'title': 'Gemini Asset Management', 'text': 'Serving high-net-worth clients in Asia, Gemini Asset Management has recently been spotlighted for unusually high commissions and inconsistent portfolio reporting. These anomalies have sparked concerns over potential money laundering and fraudulent practices.'}, {'doc_id': 2, 'title': 'Blue Horizon Investments', 'text': 'Based in London, Blue Horizon Investments is known for its innovative portfolio strategies. However, irregular transaction patterns and rapid, unexplained fund movements have raised concerns about possible insider trading and manipulation.'}, {'doc_id': 11, 'title': 'Kepler Financial Innovations', 'text': 'Kepler Financial Innovations is renowned for its ethical practices and state-of-the-art risk management. With a focus on transparency and regulatory compliance, the firm has built a reputation as a trusted player in the financial market.'}, {'doc_id': 6, 'title': 'Helix Fintech Solutions', 'text': 'Helix Fintech Solutions is a rapidly growing startup in the financial technology space. Its strategy of partnering with numerous unregulated advisors has resulted in a patchwork of compliance practices, prompting internal reviews for potential fraud and mismanagement of client funds.'}]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for value in indices_list:\n",
    "    documents.append(retrieve_document(value))\n",
    "print(f\"Top 5 documents for query '{queries[0]}':\")\n",
    "print(\"documents:\\n\", documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title: Cascade Capital Management\\nText: Cascade Capital Management, a venture capital firm specializing in tech investments, has grown rapidly but relies on a complex network of subsidiary shell companies. This structure has attracted regulatory scrutiny regarding transparency and compliance.\\n', 'Title: Gemini Asset Management\\nText: Serving high-net-worth clients in Asia, Gemini Asset Management has recently been spotlighted for unusually high commissions and inconsistent portfolio reporting. These anomalies have sparked concerns over potential money laundering and fraudulent practices.\\n', 'Title: Blue Horizon Investments\\nText: Based in London, Blue Horizon Investments is known for its innovative portfolio strategies. However, irregular transaction patterns and rapid, unexplained fund movements have raised concerns about possible insider trading and manipulation.\\n', 'Title: Kepler Financial Innovations\\nText: Kepler Financial Innovations is renowned for its ethical practices and state-of-the-art risk management. With a focus on transparency and regulatory compliance, the firm has built a reputation as a trusted player in the financial market.\\n', 'Title: Helix Fintech Solutions\\nText: Helix Fintech Solutions is a rapidly growing startup in the financial technology space. Its strategy of partnering with numerous unregulated advisors has resulted in a patchwork of compliance practices, prompting internal reviews for potential fraud and mismanagement of client funds.\\n']\n"
     ]
    }
   ],
   "source": [
    "context = []\n",
    "for doc in documents:\n",
    "    context.append(f\"Title: {doc['title']}\\nText: {doc['text']}\\n\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Creation\n",
    "\n",
    "### FAISS Index Creation:\n",
    "     The cell initializes the FAISS index by calling the `create_faiss_index()` function.  \n",
    "     This function reads and preprocesses the dataset, generates embeddings using a SentenceTransformer model, and builds a FAISS index with L2 distance.  \n",
    "     It also returns a mapping (`id_to_docs`) from index positions to document details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "index, id_to_docs = create_faiss_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Query Function Details\n",
    "\n",
    "This cell defines the `search_query` function, which is responsible for retrieving the top matching document snippets based on a user query. The key steps include:\n",
    "\n",
    "1. **Query Encoding:**\n",
    "   - Uses the same pre-trained SentenceTransformer model (`BAAI/bge-small-en-v1.5`) to encode the input query into a vector representation.\n",
    "\n",
    "2. **Similarity Search:**\n",
    "   - Performs a FAISS search on the pre-built index using the query embedding.\n",
    "   - Retrieves the top `k` similar documents from the index based on Euclidean (L2) distance.\n",
    "\n",
    "3. **Results Display:**\n",
    "   - Iterates over the retrieved results and prints each document's rank, title, description, and distance.\n",
    "   - Aggregates the title and description into a context list for potential further use.\n",
    "\n",
    "> **Note:**  \n",
    "> This function is optimized for a conversational AI context, enabling users to ask questions and receive relevant document snippets as answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Query Funcion for Index\n",
    "#  This function takes a query string, encodes it using the same model used for embedding the documents,\n",
    "#  and performs a similarity search in the FAISS index to retrieve the top k most similar documents.\n",
    "#  It returns the titles and descriptions of the top k documents along with their distances from the query.\n",
    "#  The function also prints the results in a readable format.\n",
    "#  The function is designed to be used in a conversational AI context, where the user can ask questions\n",
    "#  and receive relevant document snippets as answers.\n",
    "#  ========================================================================\n",
    "#  Search Query Function\n",
    "#  ========================================================================\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "\n",
    "def search_query(\n",
    "    index: faiss.Index,\n",
    "    id_to_doc: Dict[int, Dict[str, Any]],\n",
    "    query: str,\n",
    "    model: SentenceTransformer = SentenceTransformer(\"BAAI/bge-small-en-v1.5\"),\n",
    "    top_k: int = 3,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Encode the query, perform FAISS similarity search, and return top matching contexts.\n",
    "\n",
    "    Args:\n",
    "        index (faiss.Index): The FAISS index containing document embeddings.\n",
    "        id_to_doc (Dict[int, Dict[str, Any]]): Mapping from index position to document.\n",
    "        query (str): User query string.\n",
    "        model (SentenceTransformer): Preloaded SentenceTransformer model.\n",
    "        top_k (int): Number of top results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of top document contexts as strings.\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query]).astype(\"float32\")\n",
    "\n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # print(\"=======================================================================\")\n",
    "    # print(f\"Top {top_k} results retrieved for the Query: {query}\")\n",
    "    # print(\"=======================================================================\")\n",
    "\n",
    "    context_for_print = []\n",
    "    context: List[str] = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        title = id_to_doc[idx][\"title\"]\n",
    "        description = id_to_doc[idx][\"text\"]\n",
    "        distance = distances[0][i]\n",
    "        context_for_print.append((f\"Rank {i + 1}:\\n\", f\"Title: {title}\\n\", f\"Description: {description}\\n\", f\"Distance: {distance:.4f}\\n\\n\"))\n",
    "\n",
    "        # print(f\"Rank {i + 1}:\")\n",
    "        # print(f\"Title: {title}\")\n",
    "        # print(f\"Description: {description}\")\n",
    "        # print(f\"Distance: {distance:.4f}\\n\")\n",
    "\n",
    "        context.append(f\"Title: {title}\")\n",
    "        context.append(f\"Description: {description}\")\n",
    "    \n",
    "    # print(\"=======================================================================\")\n",
    "    return context, context_for_print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Generation Strategies\n",
    "\n",
    "This cell introduces multiple strategies for constructing prompts tailored to various language model inference tasks. The design is intended to offer flexibility and control over how context and queries are combined, which is especially valuable when fine-tuning interactions with large language models.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **PromptStyle Enum:**  \n",
    "  An enumeration (`PromptStyle`) is defined to encapsulate different prompt formatting strategies, including:\n",
    "  - **STANDARD:** Basic prompt combining context and query.\n",
    "  - **FEW_SHOT:** Adds a few recent Q&A examples to guide the model.\n",
    "  - **CHAIN_OF_THOUGHT:** Encourages step-by-step reasoning.\n",
    "  - **ROLE_BASED:** Frames the query in the context of domain expertise (e.g., forensic investigator).\n",
    "  - **BULLET_POINTS:** Instructs the model to summarize findings in bullet points.\n",
    "  - **SCORING:** Requests the model to score each organization based on risk and provide explanations.\n",
    "  - **CHATML:** Utilizes a ChatML-style formatting for models that require it.\n",
    "\n",
    "- **generate_prompt Function:**  \n",
    "  This function constructs the final prompt by:\n",
    "  1. **Aggregating Context:**  \n",
    "     Joins multiple context documents with clear separation.\n",
    "  2. **Handling Optional History:**  \n",
    "     For few-shot prompting, it appends recent Q&A pairs to enrich the prompt.\n",
    "  3. **Conditionally Formatting the Prompt:**  \n",
    "     Checks the selected `prompt_style` and formats the prompt accordingly, ensuring:\n",
    "     - **Clarity & Structure:** Each version clearly lays out the context, query, and expected model behavior.\n",
    "     - **Adaptability:** Different styles serve different purposes depending on the inference task at hand.\n",
    "\n",
    "This modular approach allows data scientists to experiment with and select the most effective prompting style for their specific use case, helping optimize the quality and relevance of model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class PromptStyle(Enum):\n",
    "    STANDARD = \"standard\"\n",
    "    # FEW_SHOT = \"few_shot\"\n",
    "    CHAIN_OF_THOUGHT = \"chain_of_thought\"\n",
    "    ROLE_BASED = \"role_based\"\n",
    "    BULLET_POINTS = \"bullet_points\"\n",
    "    CHATML = \"chatml\"\n",
    "\n",
    "\n",
    "def generate_prompt(\n",
    "    query: str,\n",
    "    context_docs: List[str],\n",
    "    prompt_style: PromptStyle = PromptStyle.STANDARD,\n",
    "    # history: List[Tuple[str, str]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt for the LLM based on the selected prompt style.\n",
    "\n",
    "    Args:\n",
    "        query (str): User's current query.\n",
    "        context_docs (List[str]): Retrieved documents for context.\n",
    "        prompt_style (PromptStyle): The strategy for prompt formatting.\n",
    "        history (List[Tuple[str, str]]): Optional memory of previous Q&A for few-shot examples.\n",
    "\n",
    "    Returns:\n",
    "        str: The final prompt to be sent to the LLM.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "    if prompt_style == PromptStyle.STANDARD:\n",
    "        return (\n",
    "            \"Your task is to analyse the question based on the context, and then provide an appropriate answer.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "\n",
    "    # if prompt_style == PromptStyle.FEW_SHOT:\n",
    "    #     few_shot_examples = \"\"\n",
    "    #     if history:\n",
    "    #         for past_q, past_a in history[-2:]:  # last 2 examples\n",
    "    #             few_shot_examples += (\n",
    "    #                 f\"Context: [Previous Retrieval]\\n\"\n",
    "    #                 f\"Question: {past_q}\\n\"\n",
    "    #                 f\"Answer: {past_a}\\n\\n\"\n",
    "    #             )\n",
    "        # return (\n",
    "        #     f\"{few_shot_examples}\"\n",
    "        #     f\"Context:\\n{context}\\n\\n\"\n",
    "        #     f\"Question: {query}\\n\\n\"\n",
    "        #     f\"Answer:\"\n",
    "        # )\n",
    "\n",
    "    if prompt_style == PromptStyle.CHAIN_OF_THOUGHT:\n",
    "        return (\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"Let's think step by step:\"\n",
    "        )\n",
    "\n",
    "    if prompt_style == PromptStyle.ROLE_BASED:\n",
    "        return (\n",
    "            f\"You are a senior forensic investigator specializing in financial crime.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Analyze the above organizations for potential risk indicators.\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "\n",
    "    if prompt_style == PromptStyle.BULLET_POINTS:\n",
    "        return (\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"List your findings in bullet points:\\n\"\n",
    "            f\"- \"\n",
    "        )\n",
    "\n",
    "    if prompt_style == PromptStyle.CHATML:\n",
    "        return (\n",
    "            f\"You are a compliance analyst. Based on the context, answer the query thoroughly.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Unsupported prompt style: {prompt_style}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Construction and Mistral Inference Details\n",
    "\n",
    "This cell defines the functions responsible for constructing a prompt for the language model, formatting the prompt for Mistral-Instruct models, and invoking the Hugging Face Inference API to generate a response. The main functions are:\n",
    "\n",
    "1. **build_prompt:**  \n",
    "   - Combines the user's query and the retrieved context into a unified prompt.\n",
    "   - The prompt instructs the model to analyze the context and answer the question.\n",
    "\n",
    "2. **format_chat_prompt:**  \n",
    "   - Wraps the prompt in a ChatML-styled template, which is required by Mistral-Instruct models for proper formatting.\n",
    "\n",
    "3. **call_mistral_hf:**  \n",
    "   - Sends the formatted prompt to the Hugging Face API endpoint for the Mistral model.\n",
    "   - Sets parameters such as temperature and max token output.\n",
    "   - Uses the API token from the environment to authenticate the request.\n",
    "   - Parses and returns the generated text from the API response.\n",
    "\n",
    "> **Note:**  \n",
    "> Ensure that your environment variable `HUGGINGFACE_API_TOKEN` is correctly set in the `.env` file before executing this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Prompt, Format chat prompt and Call Mistral HF\n",
    "#  This cell defines functions to build a prompt for a language model, format it for Mistral-Instruct models,\n",
    "#  and call the Hugging Face Inference API to generate a response.\n",
    "import os\n",
    "from typing import Optional\n",
    "import requests\n",
    "\n",
    "def build_prompt(query: str, context: str,prompt_style : str = \"standard\") -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt for language model inference based on the given query and context.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query or question.\n",
    "        context (str): Context retrieved from the documents.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted prompt string.\n",
    "    \"\"\"\n",
    "\n",
    "    # if prompt_style not in [\"standard\",\"chain_of_thought\",\"role_based\",\"bullet_points\",\"chatml\"]:\n",
    "    #     raise ValueError(f\"Unsupported prompt style: {prompt_style}\")\n",
    "\n",
    "    # Generate the prompt based on the selected style\n",
    "    if prompt_style == \"standard\":  \n",
    "        prompt_style = PromptStyle.STANDARD\n",
    "    elif prompt_style == \"chain_of_thought\":\n",
    "        prompt_style = PromptStyle.CHAIN_OF_THOUGHT\n",
    "    elif prompt_style == \"role_based\":  \n",
    "        prompt_style = PromptStyle.ROLE_BASED\n",
    "    elif prompt_style == \"bullet_points\":\n",
    "        prompt_style = PromptStyle.BULLET_POINTS\n",
    "    elif prompt_style == \"chatml\":  \n",
    "        prompt_style = PromptStyle.CHATML\n",
    "    else:\n",
    "        prompt_style = PromptStyle.STANDARD\n",
    "    \n",
    "    # Generate the prompt\n",
    "    prompt = generate_prompt(\n",
    "    query=query,\n",
    "    context_docs=context,\n",
    "    prompt_style=prompt_style\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def format_chat_prompt(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Format a prompt using ChatML-style for Mistral-Instruct models.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user input to be wrapped.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted prompt suitable for Mistral models.\n",
    "    \"\"\"\n",
    "    return f\"<s>[INST] {prompt.strip()} [/INST]\"\n",
    "\n",
    "\n",
    "def call_mistral_hf(prompt: str, api_token: Optional[str] = os.getenv(\"HUGGINGFACE_API_TOKEN\")) -> str:\n",
    "    \"\"\"\n",
    "    Call the Hugging Face Inference API for the Mistral model with the given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for generation.\n",
    "        api_token (Optional[str]): Hugging Face API token. If not provided, will read from env.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the model.\n",
    "    \"\"\"\n",
    "    if api_token is None:\n",
    "        api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "\n",
    "    if not api_token:\n",
    "        raise ValueError(\"Hugging Face API token not found. Please set 'HUGGINGFACE_API_TOKEN' in the environment.\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    formatted_prompt = format_chat_prompt(prompt)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": formatted_prompt,\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"do_sample\": True,\n",
    "            \"return_full_text\": False,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Make the API call to the Mistral model\n",
    "    api_url = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    result = response.json()\n",
    "\n",
    "    return result[0][\"generated_text\"].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Inference Function Details\n",
    "\n",
    "Next cells  defines the `inference` function, which ties together all previous functions to run end-to-end inference for a given query. The main steps are:\n",
    "\n",
    "1. **Retrieve Context:**  \n",
    "   - Calls the `search_query` function with the FAISS index and document mapping to retrieve relevant document snippets based on the query.\n",
    "   - Combines the retrieved snippets into a single context string.\n",
    "\n",
    "2. **Prompt Construction:**  \n",
    "   - Utilizes the `build_prompt` function to create a prompt that includes both the query and the context.\n",
    "  \n",
    "3. **Response Generation:**  \n",
    "   - Calls the `call_mistral_hf` function to send the prompt to the Hugging Face Inference API for the Mistral model.\n",
    "   - Returns the generated response from the model.\n",
    "\n",
    "This function encapsulates the complete workflow of the RAG system, making it straightforward to process a user query by retrieving relevant context and generating a human-like answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import faiss\n",
    "\n",
    "def retrieve_documents(query: str) -> Dict[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from the FAISS index and return them as a dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[int, Dict[str, Any]]: Mapping of index positions to documents.\n",
    "    \"\"\"\n",
    "    global index, id_to_docs\n",
    "    context_list,context_for_print = search_query(index, id_to_docs, query)\n",
    "    return context_list, context_for_print\n",
    "\n",
    "def inference(\n",
    "    query: str,\n",
    "    prompt_style: str = \"standard\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run end-to-end inference on a query using a FAISS index and Mistral API.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query to process.\n",
    "        index (faiss.Index): The FAISS index containing document embeddings.\n",
    "        id_to_docs (Dict[int, Dict[str, Any]]): Mapping of index positions to documents.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the language model.\n",
    "    \"\"\"\n",
    "    # context_str = \"\\n\\n\".join(context_list)\n",
    "    context_list, _ = retrieve_documents(query)\n",
    "    prompt = build_prompt(query, context_list,prompt_style=prompt_style)\n",
    "    return call_mistral_hf(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "\"Tell me about Cascade Capital Management?\",\n",
    "\"Which organizations show signs of potential money laundering through complex structures?\",\n",
    "\"What irregular transaction patterns are identified for Aurora Financial Services, and why do these raise concerns about potential money laundering?\",\n",
    "\"How do the described transaction patterns of Blue Horizon Investments hint at possible insider trading or market manipulation?\",\n",
    "\"Compare the descriptions of Falcon Secure Bank and Helix Fintech Solutions. What aspects of their operations contribute to one being perceived as having higher transparency and regulatory compliance than the other?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about Cascade Capital Management?\n",
      "=======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'id_to_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=======================================================================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m _, top_retrived_docs = \u001b[43mretrieve_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop Retrieved Documents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_retrived_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=======================================================================\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mretrieve_documents\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mRetrieve documents from the FAISS index and return them as a dictionary.\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Dict[int, Dict[str, Any]]: Mapping of index positions to documents.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m index, id_to_docs\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m context_list,context_for_print = search_query(index, \u001b[43mid_to_docs\u001b[49m, query)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m context_list, context_for_print\n",
      "\u001b[31mNameError\u001b[39m: name 'id_to_docs' is not defined"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=======================================================================\")\n",
    "    _, top_retrived_docs = retrieve_documents(query)\n",
    "    print(f\"Top Retrieved Documents: {top_retrived_docs}\\n\")\n",
    "    print(\"=======================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference on Multiple Queries\n",
    "\n",
    "This cell processes a list of queries by executing the end-to-end inference pipeline for each query. The key steps are:\n",
    "\n",
    "- **Iteration Over Queries:**  \n",
    "  Iterates through each query in the `queries` list.\n",
    "\n",
    "- **Printing Query Information:**  \n",
    "  For each query, it prints the query string and a divider for clarity.\n",
    "\n",
    "- **Inference Execution:**  \n",
    "  Calls the `inference` function with the current query, the pre-built FAISS index (`index`), and the document mapping (`id_to_docs`).  \n",
    "  This function retrieves relevant document snippets, constructs a prompt, and obtains a generated response via the Mistral model.\n",
    "\n",
    "- **Response Collection:**  \n",
    "  Appends the generated response to the `responses` list for further use or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_styles = [\"standard\",\"chain_of_thought\",\"role_based\",\"bullet_points\",\"chatml\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Generated Responses\n",
    "\n",
    "This cell is responsible for presenting the generated responses in a readable format. The steps involved are:\n",
    "\n",
    "1. **Iterating Over Responses:**  \n",
    "   - Loops through each response stored in the `responses` list.\n",
    "\n",
    "2. **Rendering Responses in HTML:**  \n",
    "   - Uses the `IPython.display` module to render each response within an HTML `<div>` container.  \n",
    "   - The HTML container is styled with `white-space: pre-wrap` to ensure proper word wrapping.\n",
    "\n",
    "This ensures that the output is easily readable and neatly formatted within the Jupyter Notebook interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Techniques for Enhanced Inference\n",
    "\n",
    "To maximize the quality and relevance of our generated responses, we have implemented multiple prompt strategies. These approaches allow us to experiment with different styles and choose the one that best aligns with the requirements of each query.\n",
    "\n",
    "- **Standard:** Basic integration of query and context.\n",
    "- **Chain-of-Thought:** Encourages detailed, step-by-step reasoning.\n",
    "- **Role-Based:** Frames the query from the perspective of a domain expert.\n",
    "- **Bullet Points:** Presents key information in a clear, succinct list format.\n",
    "- **ChatML:** Utilizes a conversational format tailored for dialog-based models.\n",
    "\n",
    "Explore these techniques below to see how each style influences the model's output!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses based on \"standard\" prompt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "responses = []\n",
    "for query in queries:\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=======================================================================\")\n",
    "\n",
    "    # Run inference for each query\n",
    "    response = inference(query,prompt_styles[0])\n",
    "\n",
    "    # To display it as wrapped HTML\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap; word-wrap: break-word;'>{response}</div>\"))\n",
    "    print(\"=======================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses based on \"chain_of_thoughts\" prompt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "responses = []\n",
    "for query in queries:\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=======================================================================\")\n",
    "\n",
    "    # Run inference for each query\n",
    "    response = inference(query,prompt_styles[1])\n",
    "\n",
    "    # To display it as wrapped HTML\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap; word-wrap: break-word;'>{response}</div>\"))\n",
    "    print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses based on \"role_based\" prompt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "responses = []\n",
    "for query in queries:\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=======================================================================\")\n",
    "\n",
    "    # Run inference for each query\n",
    "    response = inference(query,prompt_styles[2])\n",
    "\n",
    "    # To display it as wrapped HTML\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap; word-wrap: break-word;'>{response}</div>\"))\n",
    "    print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses based on \"bullet_points\" prompt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "responses = []\n",
    "for query in queries:\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=======================================================================\")\n",
    "\n",
    "    # Run inference for each query\n",
    "    response = inference(query,prompt_styles[3])\n",
    "\n",
    "    # To display it as wrapped HTML\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap; word-wrap: break-word;'>{response}</div>\"))\n",
    "    print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses based on \"chat_ml\" prompt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "responses = []\n",
    "for query in queries:\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=======================================================================\")\n",
    "\n",
    "    # Run inference for each query\n",
    "    response = inference(query,prompt_styles[4])\n",
    "\n",
    "    # To display it as wrapped HTML\n",
    "    display(HTML(f\"<div style='white-space: pre-wrap; word-wrap: break-word;'>{response}</div>\"))\n",
    "    print(\"=======================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
